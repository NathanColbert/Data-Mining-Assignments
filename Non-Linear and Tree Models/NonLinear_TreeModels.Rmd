---
title: "Assigment 5 Data Mining"
author: "Nathan Colbert"
date: "12/4/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
library(caret)
library(ISLR)
library(tree)
library(dplyr)
library(rpart)
library(gam)
library(bartMachine)
library(flam)
library(randomForest)
```
# Smooth Nonlinear Models for a Continuous Outcome

(a)
```{r}
set.seed(1460345448)

mark2 <- colSums(is.na(College) == 0)

mark2 <- sapply(College, FUN = is.numeric)

mark2["Private"] <- FALSE

mark2

College <- College [, mark2]

o_state <- createDataPartition(College$Outstate, times = 1, p = 3/4, 
                               list = FALSE)

training <- College[o_state, ]

testing <- College[-o_state, ]
```

(b)
```{r}
colnames(training)

oos_lm <- lm(Outstate ~ (. - Personal - perc.alumni - Expend) ^ 2, 
             data = training, y = TRUE)

oos_lm2 <- lm(Outstate ~ poly(Accept, 4) + Top10perc + Top25perc + PhD
              + S.F.Ratio + Expend, data = training, y = TRUE) 

oos_lm3 <- lm(Outstate ~ Accept + Accept * Enroll + Top10perc + PhD
              + S.F.Ratio + PhD * S.F.Ratio + perc.alumni, data = training,
              y = TRUE)

oos_lm4 <- lm(Outstate ~ (.) ^ 2, data = training, y = TRUE)

oos_lm5 <- lm(Outstate ~ (. - Personal - perc.alumni - Expend - Books - Enroll) ^ 2,
              data = training, y = TRUE)

oos_lm6 <- lm(Outstate ~ (. - Personal - perc.alumni - Expend - Books - Enroll
                          - PhD - Room.Board) ^ 2 + poly(Room.Board, 3)
              + poly(PhD, 2) + poly(Expend, 4) ,
              data = training, y = TRUE)

MSE_lm <- mean((testing$Outstate - predict(oos_lm, newdata = testing)) ^ 2)

MSE_lm2 <- mean((testing$Outstate - predict(oos_lm2, newdata = testing)) ^ 2)

MSE_lm3 <- mean((testing$Outstate - predict(oos_lm3, newdata = testing)) ^ 2)

MSE_lm4 <- mean((testing$Outstate - predict(oos_lm4, newdata = testing)) ^ 2)

MSE_lm5 <- mean((testing$Outstate - predict(oos_lm5, newdata = testing)) ^ 2)

MSE_lm6 <- mean((testing$Outstate - predict(oos_lm6, newdata = testing)) ^ 2)

c(MSE_lm, MSE_lm2, MSE_lm3, MSE_lm4, MSE_lm5, MSE_lm6)

```

(c)
```{r}
oos_gam <- gam(Outstate ~ (. - Personal - perc.alumni - Expend - Books - Enroll
                           - PhD - Room.Board) ^ 2 + s(Room.Board, 3) + s(PhD, 2)
               + s(Expend, 4) , data = training, y = TRUE)

yhat <- predict(oos_gam, newdata = testing)

plot(yhat, type = "l")
```
The gam plot shows the additive functions plotted together on the scale of the different linear predictors. 

(d) From the variables I chose, Expend, Room.Board, and PhD, exhibit non linear relationships with Outstate (4, 3, 2, respectively). When they were made into higher order functions instead of linear functions, the overall standard error decreased beyond including them or interacting them as linear terms.

(e)
```{r}
MSE_gam <- mean((testing$Outstate - predict(oos_gam, newdata = testing)) ^ 2)

c(MSE_lm6, MSE_gam)

((MSE_lm6 - MSE_gam) / MSE_lm6) * 100
```
In this instance, the gam model predicts nearly 2% better than the linear model. 

## Fused Lasso Additive Model

(a) A fused lasso additive model is an estimated piece wise function where the knots are adaptable to whatever data set one is using. 

(b)
```{r}
fl_y <- training$Outstate

fl_x <-training[, -8]

fl_testx <- testing[, -8]

colnames(fl_x)

oos_flam <- flam(fl_x, fl_y)

alpha <- oos_flam$all.alpha[50]

lambda <- oos_flam$all.lambda[50]

MSE_flam <- mean((testing$Outstate - predict(oos_flam, new.x = fl_testx,
                                             lambda = lambda, alpha = alpha)) ^ 2)

c(MSE_lm6, MSE_gam, MSE_flam)

((MSE_gam - MSE_flam) / MSE_gam) * 100
```
It appears in this case, with this set of predictors, the flam model has the lowest mean squared error in the testing data. It performs approximately 4% better than the gam model. 

## Tree-Based Models for a Binary Outcome

(a)
```{r}
load("~/Desktop/dataset5.RData")

mark3 <- colSums(is.na(dataset5) == 0) & sapply(dataset5, FUN = is.numeric)

dataset5 <- dataset5[, mark3]

dataset5$y <- factor(dataset5$y, levels = 0:1, labels = c("default", "paid"))


d5_split <- createDataPartition(dataset5$y, p = 3/4, list = FALSE)

d5_training <- dataset5[d5_split, ]

d5_testing <- dataset5[-d5_split, ]
```

(b)
```{r}
d5_glm <- glm(y ~ (.) ^ 2, data = d5_training,
              family = binomial(link = "logit"), y = TRUE)

yhat_glm <- predict(d5_glm, newdata = d5_testing, type = "response")

zhat_glm <- as.integer(yhat_glm > .05)
```

(c)
```{r, message=FALSE}
bagged <- randomForest(y ~ (.) ^ 2, data = d5_training,
                       na.action = na.exclude,
                       importance = TRUE)

set_bart_machine_num_cores(parallel::detectCores())

colnames(d5_training)

bart <- bartMachine(X = d5_training[, -13], y = d5_training$y,
                    replace_missing_data_with_x_j_bar = TRUE, 
                    mem_cache_for_speed = FALSE)
```

(d)
```{r}
bart$confusion_matrix

bagged$confusion

table(d5_testing$y, zhat_glm)

```
In order of succesful prediction, the bartMachine performs best, followed by the bagged model, and then distantly by the glm model. 

